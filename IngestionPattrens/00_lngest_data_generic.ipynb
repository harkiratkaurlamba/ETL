{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ec3206-537e-4b3b-8eb5-14bc0d46b6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b4a1ca-0785-4bc9-a43c-75a5ec21e810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_json = spark.read.json('dbfs:/databricks-datasets/samples/people/people.json')\n",
    "display(read_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17efa675-a300-40bd-8d4e-0485f9971e1c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":414},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766167535121}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls('/databricks-datasets/samples'))\n",
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/samples/newsgroups/20_newsgroups'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5d4d25-3cbc-4683-9821-015b4b5a862f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. schema enforcement in json?\n",
    "2. flatten json : arrays and struct\n",
    "3. create delta table\n",
    "4. dedup data ways and best one way and why\n",
    "5. Deduplication in Spark Streaming\n",
    "6. join > broadcast > hint join stragey\n",
    "7. merge logic \n",
    "8. scd 1/2/3\n",
    "9. schema registery \n",
    "10. schema enforcement\n",
    "11. mode: permissive etc\n",
    "12. read data from file using spark/sql which and why\n",
    "13. how to copy/insert/migrate data from one table to another OR file to table OR file to file, tradeoffs etc: when to create table and when not to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305a0668-7f7e-4037-8152-275a3cbd699b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "iot_json_df = spark.read.json('/databricks-datasets/iot/iot_devices.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d02572-91fa-4179-8d7d-3e80c9ab3dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(iot_json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e560a7-a25d-4674-8f77-1e4b3880cbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#generate sample json dtaa file\n",
    "import json\n",
    "\n",
    "# 1. Define a Python dict that maps to the desired JSON structure\n",
    "data = {\n",
    "    \"id\": 1,                                # simple datatype (int)\n",
    "    \"name\": \"Alice\",                        # simple datatype (string)\n",
    "    \"is_active\": True,                      # simple datatype (boolean)\n",
    "    \"score\": 95.5,                          # simple datatype (float)\n",
    "\n",
    "    # array of simple datatypes\n",
    "    \"tags\": [\"premium\", \"beta_user\", \"india\"],\n",
    "\n",
    "    # struct (object) with simple fields\n",
    "    \"profile\": {\n",
    "        \"email\": \"alice@example.com\",\n",
    "        \"age\": 30,\n",
    "        \"country\": \"IN\"\n",
    "    },\n",
    "\n",
    "    # nested struct (struct inside struct)\n",
    "    \"address\": {\n",
    "        \"line1\": \"123 Main Street\",\n",
    "        \"city\": \"New Delhi\",\n",
    "        \"postal_code\": \"110001\",\n",
    "        \"geo\": {                            # nested struct\n",
    "            \"lat\": 28.6139,\n",
    "            \"lon\": 77.2090\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # array of structs\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"ORD-1001\",\n",
    "            \"amount\": 1200.50,\n",
    "            \"items\": [\n",
    "                {\"sku\": \"SKU-1\", \"qty\": 1},\n",
    "                {\"sku\": \"SKU-2\", \"qty\": 2}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"ORD-1002\",\n",
    "            \"amount\": 800.00,\n",
    "            \"items\": [\n",
    "                {\"sku\": \"SKU-3\", \"qty\": 3}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. Convert to JSON string (pretty-printed for readability)\n",
    "json_str = json.dumps(data, indent=2)\n",
    "\n",
    "# 3. Write to DBFS using dbutils.fs.put\n",
    "#    Target path *in DBFS* (visible under /dbfs/ in the driver)\n",
    "target_path = \"/Volumes/hk/samplejson/jsonfiles/nested_example.json\"\n",
    "\n",
    "dbutils.fs.put(target_path, json_str, overwrite=True)\n",
    "\n",
    "print(f\"Wrote JSON to {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b46b68-24c9-4df7-a367-ce7d0b206b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Volumes/hk/samplejson/jsonfiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9709a8cc-800d-4c4e-9edd-e3f05fa2671d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.json('/Volumes/hk/samplejson/jsonfiles/nested_example.json')\n",
    "display(df )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f9b7a58-d646-4704-9e89-a65ff8074400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CSV – Raw Ingestion (Bronze)\n",
    "**Exercise 1: Basic CSV Load + Schema Control**\n",
    "\n",
    "Scenario: You receive daily CSV drops from a vendor in ADLS. Schema sometimes breaks (extra column, wrong data type).\n",
    "\n",
    "Dataset (create mock):\n",
    "- customer_id,name,age,signup_date\n",
    "- 1,Alice,29,2024-01-01\n",
    "- 2,Bob,,2024-01-03\n",
    "- 3,Charlie,thirty,2024-01-05\n",
    "\n",
    "Tasks:\n",
    "- Load CSV with: \n",
    "- header\n",
    "- explicit schema\n",
    "- Capture corrupt records\n",
    "- Write to Bronze Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91312dc-9ec5-4e80-b5d4-adebf42a169e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1,\"Alice\",29,\"2024-01-01\"), \n",
    "    (2,\"Bob\", None,\"2024-01-03\"),\n",
    "    (3,\"Charlie\",\"thirty\",\"2024-01-05\")\n",
    "    #,(None,4,\"darcy\",87,\"2024-01-08\")\n",
    "]\n",
    "\n",
    "columns = [\"cutomer_id\", \"name\", \"age\", \"signup_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/samplecsv/customerData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d541908f-9a30-4766-b282-e33bfb986f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5d42cb-146d-4673-b656-c465cd88b37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\",\"Darcy\"],\n",
    "    \"age\": [\"29\", None, \"thirty\",20],\n",
    "    \"signup_date\": [\"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"0\"]\n",
    "\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_path = \"/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f90eac-bb02-406f-98a7-7ba586988bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv', header=True, schema=\"customer_id INT, name STRING, age STRING, signup_date DATE\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa96199-d852-4ac0-9421-7d183e2908a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2798dfae-c5b4-45e6-b0d2-740a474dce23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\",\"Darcy\"],\n",
    "    \"age\": [\"29\", None, \"thirty\",20],\n",
    "    \"signup_date\": [\"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"0\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_path = \"/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f735049-2224-4e1e-849e-ad5a105400c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"cust_id\": [1,2,3,4,5],\n",
    "    \"age\":[23,32,41,19, 21],\n",
    "    \"name\":[\"alice\",\"bob\",\"charlie\",\"daren\",\"Eugene\"],\n",
    "    \"sign_up_date\":['01-01-2018', '01-03-2024', '01-05-2024', '01-08-2024','2021-00-08']\n",
    "}\n",
    "\n",
    "# data = {\n",
    "#     \"cust_id\": [1,2,3,4],\n",
    "#     \"age\":[23,32,41,19],\n",
    "#     \"name\":[\"alice\",\"bob\",\"charlie\",\"daren\"],\n",
    "#     \"sign_up_date\":['01-01-2018', '01-03-2024', '01-05-2024', '01-08-2024']\n",
    "# }\n",
    "_sample_csv = pd.DataFrame(data)\n",
    "\n",
    "_sample_csv.to_csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\", index = False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a85ece4-871c-4b09-b05e-2f9a3a6fb3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sample_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8591ffb1-fb97-438a-9b6d-ffc5811531a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark’s CSV reader expects dates to match its default pattern yyyy−MM−dd. When you declare a DateType column and do not specify a date format.\n",
    "\n",
    "​The sign_up_date values are coming in as null because Spark cannot parse the string dates in your CSV into a DateType using its default date pattern, so it silently sets them to null in PERMISSIVE mode.\n",
    "\n",
    "If the data is in dd-MM-yyyy format (for example, 01-01-2018), which does not match the default, so parsing fails and Spark cannot convert these strings into DateType. And hence, spark will read those record as NULL\n",
    " ​\n",
    "\n",
    "With mode=\"PERMISSIVE\", Spark does not throw an error; instead, it sets the unparseable fields to null and, if configured, stores the original row in the _corrupt_record column.​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf03a02-34dc-419a-8e74-06e65cbb7fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#OPTION 1: LOAD sign_up_date as StringType and later convert to date using to_date() function\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "csv_schema_2 = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"sign_up_date\", StringType(), False),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "cust_csv_raw_op2  = spark.read.option(\"header\",\"true\")\\\n",
    "    .schema(csv_schema_2)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\\n",
    "    .csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\")\n",
    "\n",
    "\n",
    "cust_csv = cust_csv_raw_op2.select(\n",
    "    \"cust_id\",\n",
    "    \"age\",\n",
    "    \"name\",\n",
    "    to_date(\"sign_up_date\", \"dd-MM-yyyy\").alias(\"sign_up_date\")\n",
    ")\n",
    "display(cust_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac56753a-ac6d-4b3c-bf32-230eb9a572ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV with:\n",
    "# header\n",
    "# explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "csv_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"sign_up_date\", DateType(), False),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#OPTION2: use option(\"dateFormat\", \"dd-MM-yyyy\") during spark.read.csv()—it's significantly faster as parsing happens natively in the optimized CSV reader without extra transformations. Preferred for high-volume CSV data\n",
    "\n",
    "cust_csv_raw_op1  = spark.read.option(\"header\",\"true\")\\\n",
    "    .schema(csv_schema)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\\n",
    "    .option(\"dateFormat\", \"dd-MM-yyyy\")\\\n",
    "    .csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\")\n",
    "display(cust_csv_raw_op1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb4f256-ef8e-474e-82b9-d268f8661362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_csv_raw_op1.filter(col(\"_corrupt_record\").isNull()).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/delta/validRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44efe7b5-7d7e-4e4c-b4fb-5386bc592395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_csv_raw_op1.filter(col(\"_corrupt_record\").isNotNull()).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/delta/inValidRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef3c980-51e6-42c7-a6f5-98552ce42a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "whata re possible data converions and casting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf40e83-eb56-4a1a-bf3f-82ccf096c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1b9f66-bb05-4126-b761-55d72c902a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Flatten JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7a07dc-212a-4590-b3e2-6355a140531a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/databricks-datasets/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c65367-be81-448b-8b36-c89062a49db3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":1083},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766682745039}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_destination-availability_logs/json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c688ef-7501-4e68-ab6b-8692ff624556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "js = spark.read.json(\"dbfs:/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_destination-availability_logs/json\")\n",
    "display(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffd2707-f762-4e04-9854-626ed5e44ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "js = spark.read.json(\n",
    "    \"dbfs:/databricks-datasets/travel_recommendations_realtime/raw_travel_data/fs-demo_destination-availability_logs/json\"\n",
    ")\n",
    "display(js)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "715a77a0-3d9a-4cd3-8a5a-91493a6d37e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Small files are handled by increasing partitions’ output size and compacting. This includes writing data with an appropriate number of output partitions (or using \n",
    "# coalesce\n",
    "# /\n",
    "# repartition\n",
    "#  before writes), and running periodic compaction jobs that read many small files and rewrite them into fewer large ones, which improves metadata load and scan performance on data lakes\n",
    "\n",
    "\n",
    "\n",
    "# Example: batch write to Parquet with controlled number of output files\n",
    "target_num_partitions = 64  # tune based on data size & cluster\n",
    "\n",
    "df_clean = (\n",
    "    input_df\n",
    "        .filter(\"event_date >= '2025-01-01'\")\n",
    "        .select(\"user_id\", \"event_time\", \"metric1\", \"metric2\")\n",
    ")\n",
    "\n",
    "# If you're increasing partitions or want a balanced layout, use repartition (shuffle = true)\n",
    "df_to_write = df_clean.repartition(target_num_partitions)\n",
    "\n",
    "# If you're only decreasing partitions after heavy shuffles, coalesce is cheaper:\n",
    "# df_to_write = df_clean.coalesce(target_num_partitions)\n",
    "\n",
    "(\n",
    "    df_to_write\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .save(\"abfss://raw@account.dfs.core.windows.net/app/events\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81a7252-d6a1-4e3a-8654-75a15e3043dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: partitioned Parquet with controlled file count per batch\n",
    "daily_df = (\n",
    "    input_df\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "        .filter(\"event_date = '2025-12-25'\")\n",
    ")\n",
    "\n",
    "# Choose partitions so that per-partition file size is close to your target (e.g. 256–1024 MB)\n",
    "daily_df = daily_df.repartition(64, \"event_date\")\n",
    "\n",
    "(\n",
    "    daily_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(\"event_date\")\n",
    "        .format(\"parquet\")\n",
    "        .save(\"abfss://raw@account.dfs.core.windows.net/app/events\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c9414e-0d1d-4e9c-9e12-698b72f810e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "source_path = \"abfss://raw@account.dfs.core.windows.net/app/events\"\n",
    "target_path = source_path                   # in-place compaction\n",
    "num_target_files = 64                       # tune based on total volume\n",
    "\n",
    "# Optional: compact only specific partition(s)\n",
    "partition_to_compact = \"event_date = '2025-12-25'\"\n",
    "\n",
    "df = (\n",
    "    spark.read\n",
    "         .format(\"parquet\")\n",
    "         .load(source_path)\n",
    "         .where(partition_to_compact)\n",
    ")\n",
    "\n",
    "(\n",
    "    df.repartition(num_target_files)\n",
    "      .write\n",
    "      .mode(\"overwrite\")       # overwrites that partition\n",
    "      .format(\"parquet\")\n",
    "      .save(target_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3a8f3f8-9b8b-4360-aa2c-d83d81707b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_path = \"abfss://silver@account.dfs.core.windows.net/app/events_delta\"\n",
    "num_target_files = 32\n",
    "\n",
    "(\n",
    "    spark.read.format(\"delta\").load(delta_path)\n",
    "         .where(\"event_date = '2025-12-25'\")\n",
    "         .repartition(num_target_files)\n",
    "         .write\n",
    "         .option(\"dataChange\", \"false\")  # important for Delta compaction\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(delta_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33fa6583-4be3-4d01-bc7d-d844b2d0737c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- compact the whole table\n",
    "OPTIMIZE catalog.schema.events_delta;\n",
    "\n",
    "-- or compact only a partition\n",
    "OPTIMIZE catalog.schema.events_delta\n",
    "WHERE event_date = '2025-12-25';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19271c34-bc87-4df7-90b9-6c3473646eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_tbl = DeltaTable.forName(spark, \"catalog.schema.events_delta\")\n",
    "\n",
    "# Full-table compaction\n",
    "delta_tbl.optimize().executeCompaction()\n",
    "\n",
    "# Partition-specific compaction (Databricks API)\n",
    "delta_tbl.optimize().where(\"event_date = '2025-12-25'\").executeCompaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85378006-2e65-4f86-807a-49f864e6252e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- At table level\n",
    "ALTER TABLE catalog.schema.events_delta\n",
    "SET TBLPROPERTIES (\n",
    "  delta.autoOptimize.optimizeWrite = true,\n",
    "  delta.autoOptimize.autoCompact = true\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e5b0f74-4b16-4554-be37-314bd899c8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f92e5e9a-3f6a-408b-b90d-d37999ca20d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "read csv file with schema evolution/register "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d04ebb-00f2-4786-8fa5-c27a1fbb6ae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# v1 CSV (old schema)\n",
    "df_v1 = (\n",
    "    spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .schema(\"id string, name string\")     # old schema\n",
    "         .load(\"/path/to/csv/v1\")\n",
    ")\n",
    "\n",
    "# v2 CSV (new schema with extra column)\n",
    "df_v2 = (\n",
    "    spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .schema(\"id string, name string, age int\")  # new schema\n",
    "         .load(\"/path/to/csv/v2\")\n",
    ")\n",
    "\n",
    "# Normalized target schema (superset)\n",
    "target_cols = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Add missing columns with nulls\n",
    "df_v1_norm = df_v1.withColumn(\"age\", F.lit(None).cast(\"int\"))[target_cols]\n",
    "df_v2_norm = df_v2.select(target_cols)\n",
    "\n",
    "# Union all versions\n",
    "df_all = df_v1_norm.unionByName(df_v2_norm)\n",
    "\n",
    "# Register or write\n",
    "df_all.createOrReplaceTempView(\"people_csv\")  # register\n",
    "# or better: write to Delta with schema evolution\n",
    "(\n",
    "    df_all.write\n",
    "        .format(\"delta\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .mode(\"append\")\n",
    "        .save(\"/mnt/delta/people\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_lngest_data_generic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
