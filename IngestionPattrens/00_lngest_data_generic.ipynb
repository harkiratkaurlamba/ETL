{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35ec3206-537e-4b3b-8eb5-14bc0d46b6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b4a1ca-0785-4bc9-a43c-75a5ec21e810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_json = spark.read.json('dbfs:/databricks-datasets/samples/people/people.json')\n",
    "display(read_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17efa675-a300-40bd-8d4e-0485f9971e1c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":414},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766167535121}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls('/databricks-datasets/samples'))\n",
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/samples/newsgroups/20_newsgroups'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f5d4d25-3cbc-4683-9821-015b4b5a862f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. schema enforcement in json?\n",
    "2. flatten json : arrays and struct\n",
    "3. create delta table\n",
    "4. dedup data ways and best one way and why\n",
    "5. Deduplication in Spark Streaming\n",
    "6. join > broadcast > hint join stragey\n",
    "7. merge logic \n",
    "8. scd 1/2/3\n",
    "9. schema registery \n",
    "10. schema enforcement\n",
    "11. mode: permissive etc\n",
    "12. read data from file using spark/sql which and why\n",
    "13. how to copy/insert/migrate data from one table to another OR file to table OR file to file, tradeoffs etc: when to create table and when not to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305a0668-7f7e-4037-8152-275a3cbd699b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "iot_json_df = spark.read.json('/databricks-datasets/iot/iot_devices.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d02572-91fa-4179-8d7d-3e80c9ab3dae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(iot_json_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e560a7-a25d-4674-8f77-1e4b3880cbf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#generate sample json dtaa file\n",
    "import json\n",
    "\n",
    "# 1. Define a Python dict that maps to the desired JSON structure\n",
    "data = {\n",
    "    \"id\": 1,                                # simple datatype (int)\n",
    "    \"name\": \"Alice\",                        # simple datatype (string)\n",
    "    \"is_active\": True,                      # simple datatype (boolean)\n",
    "    \"score\": 95.5,                          # simple datatype (float)\n",
    "\n",
    "    # array of simple datatypes\n",
    "    \"tags\": [\"premium\", \"beta_user\", \"india\"],\n",
    "\n",
    "    # struct (object) with simple fields\n",
    "    \"profile\": {\n",
    "        \"email\": \"alice@example.com\",\n",
    "        \"age\": 30,\n",
    "        \"country\": \"IN\"\n",
    "    },\n",
    "\n",
    "    # nested struct (struct inside struct)\n",
    "    \"address\": {\n",
    "        \"line1\": \"123 Main Street\",\n",
    "        \"city\": \"New Delhi\",\n",
    "        \"postal_code\": \"110001\",\n",
    "        \"geo\": {                            # nested struct\n",
    "            \"lat\": 28.6139,\n",
    "            \"lon\": 77.2090\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # array of structs\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"ORD-1001\",\n",
    "            \"amount\": 1200.50,\n",
    "            \"items\": [\n",
    "                {\"sku\": \"SKU-1\", \"qty\": 1},\n",
    "                {\"sku\": \"SKU-2\", \"qty\": 2}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"order_id\": \"ORD-1002\",\n",
    "            \"amount\": 800.00,\n",
    "            \"items\": [\n",
    "                {\"sku\": \"SKU-3\", \"qty\": 3}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 2. Convert to JSON string (pretty-printed for readability)\n",
    "json_str = json.dumps(data, indent=2)\n",
    "\n",
    "# 3. Write to DBFS using dbutils.fs.put\n",
    "#    Target path *in DBFS* (visible under /dbfs/ in the driver)\n",
    "target_path = \"/Volumes/hk/samplejson/jsonfiles/nested_example.json\"\n",
    "\n",
    "dbutils.fs.put(target_path, json_str, overwrite=True)\n",
    "\n",
    "print(f\"Wrote JSON to {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b46b68-24c9-4df7-a367-ce7d0b206b9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Volumes/hk/samplejson/jsonfiles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9709a8cc-800d-4c4e-9edd-e3f05fa2671d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.json('/Volumes/hk/samplejson/jsonfiles/nested_example.json')\n",
    "display(df )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f9b7a58-d646-4704-9e89-a65ff8074400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CSV – Raw Ingestion (Bronze)\n",
    "**Exercise 1: Basic CSV Load + Schema Control**\n",
    "\n",
    "Scenario: You receive daily CSV drops from a vendor in ADLS. Schema sometimes breaks (extra column, wrong data type).\n",
    "\n",
    "Dataset (create mock):\n",
    "- customer_id,name,age,signup_date\n",
    "- 1,Alice,29,2024-01-01\n",
    "- 2,Bob,,2024-01-03\n",
    "- 3,Charlie,thirty,2024-01-05\n",
    "\n",
    "Tasks:\n",
    "- Load CSV with: \n",
    "- header\n",
    "- explicit schema\n",
    "- Capture corrupt records\n",
    "- Write to Bronze Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91312dc-9ec5-4e80-b5d4-adebf42a169e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1,\"Alice\",29,\"2024-01-01\"), \n",
    "    (2,\"Bob\", None,\"2024-01-03\"),\n",
    "    (3,\"Charlie\",\"thirty\",\"2024-01-05\")\n",
    "    #,(None,4,\"darcy\",87,\"2024-01-08\")\n",
    "]\n",
    "\n",
    "columns = [\"cutomer_id\", \"name\", \"age\", \"signup_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/samplecsv/customerData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d541908f-9a30-4766-b282-e33bfb986f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5d42cb-146d-4673-b656-c465cd88b37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\",\"Darcy\"],\n",
    "    \"age\": [\"29\", None, \"thirty\",20],\n",
    "    \"signup_date\": [\"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"0\"]\n",
    "\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_path = \"/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV written to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f90eac-bb02-406f-98a7-7ba586988bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv', header=True, schema=\"customer_id INT, name STRING, age STRING, signup_date DATE\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa96199-d852-4ac0-9421-7d183e2908a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2798dfae-c5b4-45e6-b0d2-740a474dce23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\",\"Darcy\"],\n",
    "    \"age\": [\"29\", None, \"thirty\",20],\n",
    "    \"signup_date\": [\"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"0\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "output_path = \"/Volumes/hk/sample/samplecsv/customer_data_Test0Date.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f735049-2224-4e1e-849e-ad5a105400c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"cust_id\": [1,2,3,4,5],\n",
    "    \"age\":[23,32,41,19, 21],\n",
    "    \"name\":[\"alice\",\"bob\",\"charlie\",\"daren\",\"Eugene\"],\n",
    "    \"sign_up_date\":['01-01-2018', '01-03-2024', '01-05-2024', '01-08-2024','2021-00-08']\n",
    "}\n",
    "\n",
    "# data = {\n",
    "#     \"cust_id\": [1,2,3,4],\n",
    "#     \"age\":[23,32,41,19],\n",
    "#     \"name\":[\"alice\",\"bob\",\"charlie\",\"daren\"],\n",
    "#     \"sign_up_date\":['01-01-2018', '01-03-2024', '01-05-2024', '01-08-2024']\n",
    "# }\n",
    "_sample_csv = pd.DataFrame(data)\n",
    "\n",
    "_sample_csv.to_csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\", index = False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a85ece4-871c-4b09-b05e-2f9a3a6fb3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_sample_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8591ffb1-fb97-438a-9b6d-ffc5811531a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark’s CSV reader expects dates to match its default pattern yyyy−MM−dd. When you declare a DateType column and do not specify a date format.\n",
    "\n",
    "​The sign_up_date values are coming in as null because Spark cannot parse the string dates in your CSV into a DateType using its default date pattern, so it silently sets them to null in PERMISSIVE mode.\n",
    "\n",
    "If the data is in dd-MM-yyyy format (for example, 01-01-2018), which does not match the default, so parsing fails and Spark cannot convert these strings into DateType. And hence, spark will read those record as NULL\n",
    " ​\n",
    "\n",
    "With mode=\"PERMISSIVE\", Spark does not throw an error; instead, it sets the unparseable fields to null and, if configured, stores the original row in the _corrupt_record column.​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf03a02-34dc-419a-8e74-06e65cbb7fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#OPTION 1: LOAD sign_up_date as StringType and later convert to date using to_date() function\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "csv_schema_2 = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"sign_up_date\", StringType(), False),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "cust_csv_raw_op2  = spark.read.option(\"header\",\"true\")\\\n",
    "    .schema(csv_schema_2)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\\n",
    "    .csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\")\n",
    "\n",
    "\n",
    "cust_csv = cust_csv_raw_op2.select(\n",
    "    \"cust_id\",\n",
    "    \"age\",\n",
    "    \"name\",\n",
    "    to_date(\"sign_up_date\", \"dd-MM-yyyy\").alias(\"sign_up_date\")\n",
    ")\n",
    "display(cust_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac56753a-ac6d-4b3c-bf32-230eb9a572ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load CSV with:\n",
    "# header\n",
    "# explicit schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "csv_schema = StructType([\n",
    "    StructField(\"cust_id\", IntegerType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"sign_up_date\", DateType(), False),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#OPTION2: use option(\"dateFormat\", \"dd-MM-yyyy\") during spark.read.csv()—it's significantly faster as parsing happens natively in the optimized CSV reader without extra transformations. Preferred for high-volume CSV data\n",
    "\n",
    "cust_csv_raw_op1  = spark.read.option(\"header\",\"true\")\\\n",
    "    .schema(csv_schema)\\\n",
    "    .option(\"mode\",\"PERMISSIVE\")\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\\n",
    "    .option(\"dateFormat\", \"dd-MM-yyyy\")\\\n",
    "    .csv(\"/Volumes/hk/sample/samplecsv/customer_data.csv\")\n",
    "display(cust_csv_raw_op1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb4f256-ef8e-474e-82b9-d268f8661362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_csv_raw_op1.filter(col(\"_corrupt_record\").isNull()).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/delta/validRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44efe7b5-7d7e-4e4c-b4fb-5386bc592395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust_csv_raw_op1.filter(col(\"_corrupt_record\").isNotNull()).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/Volumes/hk/sample/delta/inValidRecords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef3c980-51e6-42c7-a6f5-98552ce42a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "whata re possible data converions and casting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf40e83-eb56-4a1a-bf3f-82ccf096c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1b9f66-bb05-4126-b761-55d72c902a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Flatten JSON"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "00_lngest_data_generic",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
