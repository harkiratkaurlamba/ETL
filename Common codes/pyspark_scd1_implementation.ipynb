{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54114b23-9969-4c94-9957-fd950b979e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, md5, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCD1\").getOrCreate()\n",
    "\n",
    "def scd1_upsert(source_df, target_path, join_keys, update_cols=\"*\"):\n",
    "    \"\"\"\n",
    "    Performs SCD1 upsert: inserts new records, updates changed ones.\n",
    "    \n",
    "    Args:\n",
    "        source_df: Incoming source DataFrame\n",
    "        target_path: Delta table path\n",
    "        join_keys: List of natural key columns (e.g., ['customer_id'])\n",
    "        update_cols: Columns to update (default all)\n",
    "    \"\"\"\n",
    "    target_df = spark.read.format(\"delta\").load(target_path)\n",
    "    \n",
    "    # Compute hash for change detection (exclude metadata)\n",
    "    hash_cols = [c for c in source_df.columns if c not in join_keys + ['load_ts']]\n",
    "    source_df = source_df.withColumn(\"hash\", md5(concat_ws(\"|\", *[col(c) for c in hash_cols])))\n",
    "    target_df = target_df.withColumn(\"hash\", md5(concat_ws(\"|\", *[col(c) for c in hash_cols])))\n",
    "    \n",
    "    # Full outer join to find new/updated records\n",
    "    join_cond = [col(f\"source.{k}\") == col(f\"target.{k}\") for k in join_keys]\n",
    "    merged_df = target_df.alias(\"target\").join(\n",
    "        source_df.alias(\"source\"), join_cond, \"full_outer\"\n",
    "    )\n",
    "    \n",
    "    # New records: target key null\n",
    "    new_records = merged_df.filter(col(\"target.\" + join_keys[0]).isNull()).select([col(\"source.\" + c).alias(c) for c in source_df.columns])\n",
    "    \n",
    "    # Updated records: hash mismatch\n",
    "    updated_records = merged_df.filter(\n",
    "        col(\"source.\" + join_keys[0]).isNotNull() & \n",
    "        (col(\"source.hash\") != col(\"target.hash\"))\n",
    "    ).select([col(\"source.\" + c).alias(c) for c in source_df.columns])\n",
    "    \n",
    "    # Combine new + updated\n",
    "    upsert_df = new_records.unionByName(updated_records)\n",
    "    upsert_df = upsert_df.withColumn(\"load_ts\", current_timestamp())\n",
    "    \n",
    "    # Write as upsert to Delta\n",
    "    delta_table = DeltaTable.forPath(spark, target_path)\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        upsert_df.alias(\"source\"), join_cond\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Usage example\n",
    "source_df = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"new_email@example.com\"),\n",
    "    (2, \"Jane Smith\", \"jane@example.com\")\n",
    "], [\"id\", \"name\", \"email\"])\n",
    "\n",
    "scd1_upsert(source_df, \"/path/to/delta/customer_dim\", [\"id\"])\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_scd1_implementation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
