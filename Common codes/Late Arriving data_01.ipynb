{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2cc08cf-6c58-4631-afa9-96bbec29daea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 1: Reprocessing Window (Simplest & Common)\n",
    "\n",
    "Always reprocess last N days so late data automatically gets picked up.\n",
    "Use when\n",
    "- Late data is bounded (e.g. max 2–7 days)\n",
    "- Volume is manageable\n",
    "- Aggregates can be recomputed cheaply\n",
    "\n",
    "Tradeoff\n",
    "- Rewrites partitions every run\n",
    "- Not ideal if data is huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a59d649-96c4-48cb-9130-2a69e476526e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, date_sub\n",
    "\n",
    "# Reprocess last 3 days\n",
    "reprocess_days = 3\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"/bronze/events\")\n",
    "\n",
    "filtered_df = df.filter(\n",
    "    df.event_date >= date_sub(current_date(), reprocess_days)\n",
    ")\n",
    "\n",
    "filtered_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"replaceWhere\", f\"event_date >= date_sub(current_date(), {reprocess_days})\") \\\n",
    "    .save(\"/silver/events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ec29cd-afff-4ea3-955f-03b42720ac3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 2: Event-Time Partitioning + Append (Bronze)\n",
    "\n",
    "Late data is written to the correct partition, not “today’s” partition.\n",
    "\n",
    "Use when\n",
    "- You want immutability\n",
    "- Late data is frequent\n",
    "- You don’t want special handling logic\n",
    "\n",
    "**Key point** \n",
    "Late data for Jan 22 arriving on Jan 24 still lands in event_date=2024-01-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93ebe06-d77f-42e5-a1a3-b7d22ea1c529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/landing/events\")\n",
    "\n",
    "df = df.withColumn(\"event_date\", df.event_timestamp.cast(\"date\"))\n",
    "\n",
    "df.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(\"/bronze/events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe79a7e-81c5-4561-9837-3865534ca70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 3: MERGE (Upsert) for Late & Corrected Data (Silver)\n",
    "\n",
    "Late data updates or inserts records instead of duplicating them.\n",
    "\n",
    "Use when\n",
    "- Data has natural keys\n",
    "- Updates are expected\n",
    "- You need correctness over time\n",
    "\n",
    "Pros:\n",
    "- Idempotent\n",
    "- Safe re-runs\n",
    "- Late data naturally handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8146a1b-1d09-45fd-8cfc-c789b22a92bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "silver = DeltaTable.forPath(spark, \"/silver/events\")\n",
    "\n",
    "updates = spark.read.format(\"delta\").load(\"/bronze/events\")\n",
    "\n",
    "silver.alias(\"t\").merge(\n",
    "    updates.alias(\"s\"),\n",
    "    \"t.event_id = s.event_id\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a30c36-f006-4685-b0f9-b3ea917be322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 4: Separate Late Data Detection (Explicit Handling)\n",
    "\n",
    "Detect late data and process it explicitly, not implicitly.\n",
    "\n",
    "Use when\n",
    "- SLA-driven systems\n",
    "- Audits required\n",
    "- Late data is an exception, not the norm\n",
    "\n",
    "Risk\n",
    "- Extra complexity\n",
    "- Must merge downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69a403e-6fcd-4b87-a29e-ed98ca49b106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "late_threshold_hours = 24\n",
    "\n",
    "df = spark.read.json(\"/landing/events\")\n",
    "\n",
    "on_time = df.filter(\n",
    "    df.ingest_time >= df.event_time\n",
    ")\n",
    "\n",
    "late = df.filter(\n",
    "    df.ingest_time < df.event_time\n",
    ")\n",
    "\n",
    "on_time.write.format(\"delta\").mode(\"append\").save(\"/bronze/on_time\")\n",
    "late.write.format(\"delta\").mode(\"append\").save(\"/bronze/late_events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e9b3e8-f353-4ecf-bcd2-16ffa313e444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Controlled late processing\n",
    "late_events = spark.read.format(\"delta\").load(\"/bronze/late_events\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903684b0-f59d-4972-90f6-b8696d5cb852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 5: Watermark + Backfill Trigger (ADF Friendly)\n",
    "\n",
    "Process normally using watermark, trigger backfill when late data detected.\n",
    "\n",
    "Use when\n",
    "- Batch systems\n",
    "- ADF-based orchestration\n",
    "- Business-approved reprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8215c502-72ba-4134-a98b-5540475ce980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_watermark = \"2024-01-24 10:00:00\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"/bronze/events\")\n",
    "\n",
    "incremental = df.filter(df.updated_at > last_watermark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e835c1-0b82-41e7-8ac4-b483fe509fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "late_data = df.filter(df.updated_at <= last_watermark)\n",
    "\n",
    "if late_data.count() > 0:\n",
    "    # Trigger backfill pipeline via ADF\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd27b751-ee37-4a72-bb8e-d1f551535a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pattern 6: Gold Layer – Correcting Aggregates (MOST MISSED)\n",
    "\n",
    "Never patch aggregates. Recompute affected partitions only.\n",
    "\n",
    "Use when\n",
    "- Metrics matter\n",
    "- Late data impacts KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9fd4a24-da5f-4c27-a7e6-9a59a5182f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "affected_dates = (\n",
    "    late_data\n",
    "    .select(\"event_date\")\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "for row in affected_dates:\n",
    "    date = row[\"event_date\"]\n",
    "\n",
    "    base = spark.read.format(\"delta\").load(\"/silver/events\") \\\n",
    "        .filter(f\"event_date = '{date}'\")\n",
    "\n",
    "    agg = base.groupBy(\"product_id\") \\\n",
    "        .count()\n",
    "\n",
    "    agg.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"replaceWhere\", f\"event_date = '{date}'\") \\\n",
    "        .save(\"/gold/daily_sales\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89d1fed0-5360-4199-bced-c3f569082a5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When to Use Which (Summary Table)\n",
    "\n",
    "Pattern -\tBest For\n",
    "- Reprocessing window\t- Small bounded lateness\n",
    "- Event time partitioning -\tImmutable ingestion\n",
    "- MERGE -\tUpdates + late data\n",
    "- Explicit late stream -\tSLA / audit systems\n",
    "- Watermark + backfill\t- ADF batch pipelines\n",
    "- Targeted gold recompute -\tKPI correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39ed21e-2c4c-4b9b-8e76-c3397985ef83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Late-arriving data is ingested using event-time partitioning, corrected via idempotent MERGE operations in silver, and aggregates are recomputed only for impacted partitions in gold"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Late Arriving data_01",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
